# Chapter-1
**强化学习(reinforcement learning)**是一种**以目标为导向、从与周围环境的互动中学习**的计算方法.  
   
为了使整个问题具体和简化，我们抽象出了以下**概念**：  
**智能体(agent)**
智能体**代表我们与环境进行交互**，且具有以下三个特点：  
> **能在一定程度上感知当前的环境;**  
**可以采取行动影响当前的环境;**  
**有一个或者多个与环境状态相关的目标;**  
  
**状态(state)**  
> 状态可以看做一种对环境的总结，包含了用于确定接下来会发生的事情的所有信息  
  
**动作(action)**  
> 动作是对智能体所有可以采取的行动的一种抽象，智能体可以通过动作改变当前的状态  
  
**策略(policy)**  
> 智能体的决策系统，给定当前状态，需要作出决定采取什么样的行动  
通俗来说，策略是一个由状态空间向动作空间的映射；在一般的情况下，这个映射可以是随机的(stochastic)  
  
**奖励信号(reward signal)**  
> 智能体在每作出一个动作后都会收到来自环境的奖励信号，代表着对该动作的评价反馈，评价越高奖励越大  
智能体的任务就是最大化所得到奖励的总和  
  
**值函数(value function)**  
>值函数是智能体对每一个状态的评估，代表从这个状态出发，所能得到的奖励总和的期望  
奖励信号是由环境给出的，而值函数是来自智能体的自我估计  
值函数是建立在奖励信号的基础上的，没有奖励信号就没有值函数  
与奖励信号不同的是，值函数刻画了这个状态长期意义下的好坏程度；而奖励信号是短期意义下的好坏  
很大程度上我们可以直接参考值函数来做决策  
正因如此， 对于大部分强化学习算法，其中最重要的就是对值函数的有效估计  
   
**环境模型(model)**  
> 出于对人类推断(inference)能力的模仿以及对在与环境交互时代价的考虑，部分强化学习算法会考虑让智能体对其交互的环境进行建模，也就是环境模型  
在与实际环境交互前，智能体可以先于其内部的环境模型进行交互，从而实现推断和预测实际环境下一步反馈的目的  
其中与环境模型交互的部分称为规划(planning)  

依据是否建立环境模型，强化学习算法可以被划分成两大类：  
>基于模型(model-based)的强化学习  
模型无关(model-free)的强化学习  
(基于模型的算法往往更加复杂)  
  
强化学习最突出的**两大特征**：  
> 不断地尝试和纠错(试错搜索 / **trial-and-erro**r search)  
	> 在不被告知有哪些可以执行的动作的情况下，算法需要自己去发现可能的动作，并且对每一个动作给出一个好坏程度的评估  
> 回报可能是延迟的(**延迟回报 / delayed reward**)  
	> 一个动作，除了会影响我们当前得到的回报外，也有可能影响之后行动的回报  

本书介绍的**强化学习方法与一系列进化方法(evolutionary methods)的区别**：  
> * 进化方法  
	* 进化方法(比如遗传算法、模拟退火)，往往更加关心宏观上的表现，算法会产生大量的智能体，让这些智能体各自去与环境交互，当交互结束后，按照一定的评价方法和筛选标准，留下那些可能比更优的智能体，进行下一轮迭代  
	* 即每个智能体从与环境开始交互开始到结束，我们只在最后对其进行一次评估，并不关心在交互过程中每个状态或者行动直接的关系与影响  
  
> * 本书讨论的强化学习方法  
	* 更加注重于智能体与环境互动的过程中所发生的事情，利用到了策略其实是一个由状态到动作的映射这一事实，往往这样的算法的学习效率更高  
	* 即智能体在与环境开始交互到结束为止，会去估计每一个到过的状态或者执行过的动作，对最终达到的结果有多大的贡献，这样可以得到每个状态和动作更加精确的value，提升今后决策的可靠性  
   
> 值得指出的是，进化方法在小规模问题，或者算力足够，以及在环境的状态只能够被部分观察到的情况下，较后者会更有优势  
  
**强化学习与监督学习(supervised learning)和无监督学习(unsupervised learning)的区别：**  
> * 强化学习与监督学习  
	* 监督学习尝试从一些表示(当前状态, 应该采取的行动)的对应中，泛化出能够在遇到之前没有见过的状态时，做出正确的行动的能力  
	* 强化学习则侧重于通过与环境的互动，去探索不同状态下可以执行的动作，从自己与环境互动的历史中学习，通过估价来选择出最优的动作  
> * 强化学习与无监督学习  
	* 无监督学习尝试找出给定的数据中隐含的内在结构与关联  
	* 强化学习唯一的目标就是最大化得到奖励的总和，至于是否找到了数据的某种内在关联，我们并不关心  
	  
**强化学习中特有的问题：探索(exploration)与利用(exploitation)的平衡**  
> * 所谓探索就是指主动去发现新的信息，使得当前对动作的估价更加准确，以去及发现新的动作  
* 所谓利用就是利用当前已知的所有信息，找到最佳的动作序列，达到最大化总回报的目标
* 一味的探索就需要花费大量的尝试来估计每个动作的好坏，而一味的利用则会错过发现更好的动作的机会，从而陷入局部最优