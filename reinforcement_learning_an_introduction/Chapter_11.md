# 基于函数逼近的离轨策略方法  
**同轨策略方法中**,智能体一直波爱吃试探并尝试寻找也能继续保持试探的最优策略.   
**离轨策略方法中**,虽然智能体也在试探,但它实际学习的可能是与它试探时使用的策略无关的另一个确定性策略.   

同轨策略和离轨策略都是为了解决学习中试探与开发的内在冲突.    

**本章内容**:   
> 考虑了离轨策略方法中的**收敛性问题**;   
> 对于线性函数逼近,介绍了"**可学习程度**" 的概念.   
> 讨论了**离轨策略情形下的一些新算法**.   
  
**离轨策略学习目前面临的两个问题(挑战)**:   
> 一部分出现在**表格型**情况.   
> 另一部分出现在**函数近似中**.

> 第一部分需要**处理新的目标.**(注意不要与目标策略混淆)   
> 第二部分需要**处理更新的分布**.   

**针对挑战的解决办法**:   
> **应对第一种挑战**:  
> (第5,7章中提到的)**重要度采样技术**.   

> **应对第二种挑战**: (有两种通用的方式)   
> 1. 是再次使用重要度采样方法，扭曲更新后的分布将其变回到同轨策略上的分布，以便保证半梯度方法收敛（在线性情况下）。   
> 2. 寻找一种不依赖于任何特殊分布就能稳定的真实梯度方法.   

##  11.1 半梯度方法  
本节首先描述在前面的章节中为离轨策略案例开发的方法作为半梯度方法，如何容易地扩展到函数逼近中。   

**重要度采样率：**   
![0](/home/tenglong/0.png)   

**单步状态价值算法就是半梯度离策略TD(0)**:   
![1](/home/tenglong/0.png)   

**对于动作价值函数，单步算法就是半梯度预期Sarsa算法：**   
![2](/home/tenglong/0.png)    
 >**注意**：   
 > **此算法不使用重要性采样**。   
 在表格情况下，很明显这是恰当的，因为唯一的示例动作是 At， 在学习它的价值时，我们不必考虑任何其他动作。  对于函数近似，它不太清楚，因为一旦它们都对相同的整体近似有贡献，我们可能想要对不同的状态-动作对进行不同的加权。   
 > 正确解决这个问题等待对强化学习中函数近似理论的更透彻理解。   
 
 **在这些算法的多步泛化中，状态价值和动作价值算法都涉及重要性采样:**  
 ![3](/home/tenglong/0.png)   
 
##  11.2 离策略发散例子   
**例子1：**   
![4](/home/tenglong/0.png)    

**例子2：**   
![5](/home/tenglong/0.png)    

 **注：**   
 另一种尝试防止不稳定的方法是使用特殊方法进行函数近似。   
 > 特别是，对于不从观察到的目标推断的函数近似方法，保证了稳定性。 这些方法称为 **平均器（averagers**），包括最近邻方法和局部加权回归，但不是流行的方法，如铺片编码和人工神经网络（ANN）   
 
##  11.3 致命三要素：  
只要我们的方法同时包含以下三个要素，构成我们称之为 **致命的三元组**，就会产生不稳定和发散的危险：   
**函数近似** 从比内存和计算资源（例如，线性函数近似或ANN）大得多的状态空间泛化的强大和可扩展的方式。  

**自举** 更新目标包括现有估计（如动态编程或TD方法），而不是完全依赖实际奖励和完整回报（如MC方法）。  

**离策略训练** 除了目标策略产生的转移分布之外的训练。扫描状态空间并统一更新所有状态，如动态规划， 不遵循目标策略，是离策略训练的一个例子。   

在这三者中，**函数近似**最不可能被放弃。   
不使用 **自举** 是有可能的，但是付出的是计算和数据效率为代价。通过自举实现的通信和内存节省是非常好的。   
>  自举通常会导致更快的学习，因为它允许学习利用状态属性，即在返回状态时识别状态的能力。    
> 另一方面，自举可能会削弱对状态表示不佳的问题的学习并导致泛化效果差。    

 **离策略方法**从目标策略中释放行为。这可以被认为是一种吸引人的便利，但不是必需的。 但是，离策略学习对于其他预期的用例至关重要，我们在本书中尚未提及但可能对创建强大智能个体的更大目标很重要.   
 
##  11.4 线性价值函数几何
**线性价值函数逼近的几何性质：**  
![6](/home/tenglong/0.png)    

**用范数定义价值函数之间的距离：**   
![7](/home/tenglong/0.png)    

**投影算子 Π**：   
![8](/home/tenglong/0.png)    

**投影矩阵：**  
![9](/home/tenglong/0.png)   

**价值函数 vπ 的Bellman方程 和 Bellman误差 和 均方Bellman误差 和 Bellman算子**   
![10](/home/tenglong/0.png)   

**均方投影Bellman误差：**   
![11](/home/tenglong/0.png)   

## 