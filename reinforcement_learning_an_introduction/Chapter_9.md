# 第9章 在策略预测近似方法

在本章中，我们开始研究强化学习中的函数近似，考虑其在从在策略数据中估计状态-价值函数的用途， 即从使用已知策略 π生成的经验近似 vπ。 本章的新颖之处在于，近似值函数不是用表格表示， 而表示成是具有权重向量 w∈Rd 的参数化函数形式。 我们将权重向量 ww 的状态 ss 的近似值写作 v^(s,w)≈vπ(s)。 例如，v^ 可能是状态特征中的线性函数，w 是特征权重的向量。 更一般地，v^ 可以是由多层人工神经网络计算的函数，其中 ww 是所有层中的连接权重的向量。 通过调整权重，网络可以实现各种不同函数中的任何一种。 或者 v^ 可以是由决策树计算的函数，其中 w 是定义树的分裂点和叶值的所有数字。 通常，权重的数量（w 的维数）远小于状态的数量（d≪|S|）， 并且改变一个权重会改变许多状态的估计值。因此，当更新单个状态时，更改会从该状态推广到许多其他状态的值。 这种 *泛化* 使得学习可能更强大，但也可能更难以管理和理解。  

也许令人惊讶的是，将强化学习扩展到函数近似也使其适用于部分可观察到的问题，其中个体无法获得完整状态。 如果 v^的参数化函数形式不允许估计值依赖于状态的某些方面，那么就好像这些方面是不可观察的。 实际上，本书这一部分中使用函数近似的方法的所有理论结果同样适用于部分可观察的情况。 然而，函数近似不能做的是用过去观察的记忆来增强状态表示。第17.3节简要讨论了一些可能的进一步扩展。  

## 9.1 价值函数近似

本书中涉及的所有预测方法都被描述为对估计价值函数的更新，该函数将其在特定状态下的值转换为该状态的“备份值”或 *更新目标*。 让我们使用符号 s↦u 的表示单独更新，其中 s 是更新的状态， u 是 s 的估计价值转移到的更新目标。 例如，价值预测的蒙特卡洛更新是St↦Gt， TD(0)更新是 St↦Rt+1+γv^(St+1,wt)， n步TD更新为 St↦Gt:t+n。在DP（动态规划）中策略评估更新， s↦Eπ[Rt+1+γv^(St+1,wt)|St=s]， 任意状态 s 被更新，而在其他情况下，实际经验中遇到的状态 St 被更新。   

将每个更新解释为指定价值函数的所需输入-输出行为的示例是很自然的。 从某种意义上说，更新 s↦u 表示状态 s 的估计值应更像更新目标 u。 到目前为止，实际的更新是微不足道的：s 的估计值的表条目已经简单地转移到了 u 的一小部分， 并且所有其他状态的估计值保持不变。现在，我们允许任意复杂和精致方法来实现更新，并在 s 处进行更新，以便更改许多其他状态的估计值。 学习以这种方式模拟输入输出示例的机器学习方法称为 *监督学习* 方法，当输出是像 u 的数字时，该过程通常称为 ***函数近似***。 函数近似方法期望接收它们试图近似的函数的期望输入-输出行为的示例。 我们使用这些方法进行价值预测，只需将每次更新的 s↦u 作为训练样例传递给它们。 然后，我们将它们产生的近似函数解释为估计价值函数。   

以这种方式将每个更新视为传统的训练示例使我们能够使用任何广泛的现有函数近似方法来进行价值预测。 原则上，我们可以使用任何方法进行监督学习，例如人工神经网络，决策树和各种多元回归。 然而，并非所有函数近似方法都同样适用于强化学习。最复杂的人工神经网络和统计方法都假设一个静态训练集，在其上进行多次传递。 然而，在强化学习中，学习能够在线进行，而个体与其环境或其环境模型进行交互是很重要的。 要做到这一点，需要能够从增量获取的数据中有效学习的方法。 此外，强化学习通常需要函数近似方法能够处理非平稳目标函数（随时间变化的目标函数）。 例如，在基于GPI（广义策略迭代）的控制方法中，我们经常寻求在 π 变化时学习 qπ。 即使策略保持不变，如果训练示例的目标值是通过自举方法（DP和TD学习）生成的，则它们也是非平稳的。 不能轻易处理这种非平稳性的方法不太适合强化学习。    

## 9.2 预测目标（VE¯）  

到目前为止，我们尚未指定明确的预测目标。在表格的情况下，不需要连续测量预测质量，因为学习价值函数可以精确地等于真值函数。 此外，每个状态的学习价值都是分离的──一个状态的更新不受其他影响。 但是通过真正的近似，一个状态的更新会影响许多其他状态，并且不可能使所有状态的值完全正确。 假设我们有比权重更多的状态，所以使一个状态的估计更准确总是意味着让其他的不那么准确。 我们有义务说出我们最关心的状态。 我们必须指定状态分布μ(s)≥0,∑sμ(s)=1，表示我们关心每个状态 s 中的错误的程度。 通过状态 s 中的误差，我们指的是近似值 v^(s,w) 与 真值)vπ(s) 之间的差的平方。 通过 μ对状态空间加权，我们得到一个自然目标函数，*均方误差*，表示为VE¯：    

VE¯(w)≐∑s∈Sμ(s)[vπ(s)−v^(s,w)]2(9.1)    

该度量的平方根（根VE¯）粗略地衡量了近似值与真实值的差异，并且通常用于图中。 通常 μ(s) 被选择为 s 中花费的时间的一部分。 在在策略训练中，这被称为 ***在策略分布***；我们在本章中完全关注这个案例。 在持续任务中，在策略分布是 math:pi 下的固定分布。  

![0](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/0.png)   

这两种情况，即持续的和回合的，表现相似，但近似时必须在形式分析中单独处理， 正如我们将在本书的这一部分中反复看到的那样。这完成了学习目标的规范。   

目前还不完全清楚 VE¯ 是加强学习的正确性能目标。 请记住，我们的最终目的──我们学习价值函数的原因──是找到更好的策略。 用于此目的的最佳价值函数不一定是最小化VE¯ 。 然而，目前尚不清楚价值预测的更有用的替代目标是什么。目前，我们将专注于VE¯。   

就 VE¯ 而言，理想的目标是找到一个 *全局最优值*， 一个权重向量 w∗，对于所有可能的 w， VE¯(w∗)≤VE¯(w)。 对于诸如线性函数近似器之类的简单函数逼近器，有时可以实现这一目标， 但对于诸如人工神经网络和决策树之类的复杂函数近似器来说很少是可能的。 除此之外，复杂函数近似器可以寻求收敛到 *局部最优*，一个权重向量 w， 对于 w∗的某些邻域中的所有 w 满足VE¯(w∗)≤VE¯(w)。 虽然这种保证只是稍微让人放心，但对于非线性函数近似器来说，它通常是最好的，而且通常它就足够了。 尽管如此，对于许多对强化学习感兴趣的情况，并不能保证收敛到最佳，甚至在最佳的有界距离内。 事实上，有些方法可能会出现发散，其 VE¯ 极限趋于无穷。   

在前两节中，我们概述了一个框架，用于将价值预测的各种强化学习方法与各种函数近似方法相结合，使用前者的更新为后者生成训练样本。 我们还描述了这些方法可能希望最小化的 VE¯ 性能测量。 可能的函数近似方法的范围太大以至于不能覆盖所有方法，并且无论如何对其中的大多数方法进行可靠的评估或推荐知之甚少。 必要时，我们只考虑几种可能性。在本章的剩余部分，我们将重点放在基于梯度原理的函数近似方法，特别是线性梯度下降方法上。 我们关注这些方法的部分原因是因为我们认为这些方法特别有前途，因为它们揭示了关键的理论问题，同时也因为它们很简单，而且我们的空间有限。    

## 9.3 随机梯度和半梯度方法  
**随机梯度下降 （SGD）方法基本原理:**  
随机梯度下降 （SGD）方法通过在每个样例之后将权重向量向最大程度地减少该示例中的误差的方向少量调整来实现此目的：  
![1](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/1.png)    
**称SGD方法是“梯度下降”方法的原因**:   
>  因为 Wt 中的整个步长与示例的平方误差（9.4）的负梯度成比例。同时,这是误差下降最快的方向。   
  
  **“随机”的来源**:  
  > " 随机的 " 体现在更新仅仅依赖于一个样本完成,而且该样本很有可能还是随机选择的.  
  
  **最终的目的:**  
请记住，我们不会寻找或期望找到一个对所有状态都没有误差的价值函数， 而**只是想要一个平衡不同状态误差的近似值**。如果我们在一个步骤中完全纠正每个样例，那么我们就找不到这样的平衡。   
   
 **梯度和半梯度方法**:  
 梯度方法:  
 > **蒙特卡洛目标 **Ut≐Gt 根据定义是 vπ(St) 的**无偏估计**.  因此，蒙特卡洛状态价值预测的梯度下降版本保证找到局部最优解。   
 
 半梯度方法:  
 > **自举目标**如n步回报 Gt:t+n 或**DP目标** ∑a,s′,rπ(a|St)p(s′,r|St,a)[r+γv^(s′,wt)] 全部依赖关于权重向量 wt 的当前值，这意味着它们将被偏置并且它们将不会产生真正的梯度下降方法  
 > **自举目标**和**DP目标**都是有偏估计,我们称他们为半梯度方法.  
 > 半梯度TD(0)是一个在原型意义上的半梯度方法。  
 

**半梯度方法的优缺点**:  
**缺点:**  
>  半梯度（自举）方法不像梯度方法那样稳健地收敛，但它们在重要情况下可靠地收敛，例如下一节中讨论的线性情况。   
  
  **优点:**   
  > 一个原因是它们通常能够显着加快学习速度。   
  >  另一个原因是它们使学习能够连续和在线，而无需等待回合的结束。   
## 9.4 线性方法  
### 9.4.1 线性来源:  
近似价值函数被称为 **依据权重线性的**（linear in the weights），或简单地称为**线性**。    
即: ![2](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/2.png)   
> 向量 x(s) 被称为表示状态 s 的 **特征向量**。    x(s) 的每个分量 xi(s) 是 函数 xi:S→R (实数域)的值。   
> 我们所说的**特征**就是一个完整的函数。  
>  对于线性方法，特征被称作 **基函数**，因为它们构成了可能的近似函数集的线性基。    
### 9.4.2 线性TD(0)的收敛性证明  
![3](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/3.png)    
![4](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/4.png)   

> **线性半梯度DP**根据在策略分布进行更新也将收敛到TD固定点。  
> **一步半梯度 动作价值** 方法， 例如下一章中介绍的半梯度Sarsa(0)会收敛到类似的固定点和类似的边界。   
> 对于**回合任务**，存在一个稍微不同但相关的界限（参见Bertsekas和Tsitsiklis，1996）。   
> 这些收敛结果的**关键是根据在策略分布更新状态**。对于其他更新分布，使用函数近似的自举方法实际上可能会发散到无穷大。  
## 9.5 线性方法的特征构造  
### 9.5.1 本节讨论的内容:  
线性方法因其收敛性保证和在实践中它们在数据和计算方面都非常有效而很有意思。   
但是,在实践中它们在数据和计算方面是否非常有效的关键在于**如何选取用来表示状态的特征**.   
**选择适合任务的特征是将先验知识添加到强化学习系统的重要方法**。 直观地，这些特征特征应该提取状态空间中最通用的信息。   
### 9.5.2 多项式基:  
![5](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/5.png)   
### 9.5.3 傅里叶基:  
**对于一维情况**:  
> 具有周期 τ 的一维函数的通常傅立叶级数表示为正弦和余弦函数的线性组合的函数， 每个函数周期性地均衡分割 τ 的周期（换句话说，其频率是整数乘以基频 1/τ）。  
  
 > 但是如果你对近似有界区间定义的非周期函数感兴趣，那么你可以使用这些傅里叶基特征，并将 τ 设置为区间的长度。 因此，感兴趣的函数只是正弦和余弦特征的周期线性组合的一个周期。   
   
 **注意:**  
 > 可以只用余弦基表示任何 偶 函数，即任何与原点对称的函数。   
 > 可以只使用正弦特征，其线性组合总是 奇 函数，是关于原点反对称的函数。     
 > 但通常更好的是保持余弦特征，因为“半偶数”函数往往比“半奇”函数更容易近似，因为后者在原点通常是不连续的。    
> 当然并不意味这不能同时使用正弦和余弦来近似区间 [0,τ/2],在某些情况下同时使用是有利的.   
  
  **多维情况下的傅立叶余弦序列近似的详细描述:**  
  ![6](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/6.png)   
  
  **步长参数设置的建议:**   
  ![7](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/7.png)    
  
**注意:**   
> 傅里叶特征在不连续性方面存在问题，因为除非包括非常高频率的基函数，否则很难避免在不连续点周围的“波动”问题。   

> **n 阶傅里叶基的特征数量随着状态空间的维数呈指数增长**，但如果该尺寸足够小（例如，k≤5）， 那么可以选择 n 以便所有的 n 阶傅立叶数特征可以使用。 这使得特征的选择或多或少是自动的。但是，**对于高维状态空间，有必要选择这些特征的子集**。   

> **选取特征子集**可以引入要近似的函数的性质的先验信念来完成，并且可以调整一些自动选择方法以处理强化学习的增量和非平稳性质。    
> **傅里叶基特征在这方面的优点在于:  
> 一方面,**通过设置 ci 向量来描述状态变量之间的相互作用， 并通过限制 ci 向量中的值来近似滤除被认为是噪音的高频分量。  
>  **另一方面**，由于傅立叶特征在整个状态空间上几乎都是非零的（除了少数零），它们代表状态的全局属性，这也使得表示局部属性比较困难。    
### 9.5.4 粗编码:  
![8](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/8.png)   
如果状态在圆内，则相应的特征具有值1并且可以说是 **出席**（present）；否则该特征为0并且可以说 **缺席**（absent）。   
这种表示状态重叠性质的特征(不一定是圆或者二值)被称为**粗编码**.  
### 9.5.5 瓦片编码:  
![9](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/9.png)   
在瓦片编码中，特征的感受野组成了状态空间的一系列划分。   
每个这样的划分称为 **覆盖**（tiling），覆盖的每个元素称为一个 **瓦片**（tile）。    

**瓦片编码的一个直接优点在于**:  
> 因为它是整体区间的划分，所以任何状态在每一时刻激活的特征的总数是相同的.  

**通常人们希望更新的改变的慢一点，以允许目标输出的泛化和随机变化**.   

**注意:**  
> 在许多模式中，**均匀偏移会在对角线上产生强烈的效果**。   
> 可以通过使用**非对称偏移**来避免这一影响.  
> 一般而言非对称个偏移的泛化模式更好，因为它们一般可以很好地集中在被训练状态周围并且没有明显的不对称性。   

**关于位移向量选择的建议:**  
> Miller和Glanz（1996）建议使用由第一个奇数整数组成的位移矢量。    
>  特别是，对于维数 k 的连续空间，一个很好的选择是使用第一个奇数整数（1,3,5,7,…,2k−1）， 其中 n （覆盖的个数）设置为大于或等于 4k 的2的整数幂。   

**在实际的应用中,最好在不同的覆盖中使用不同形状的瓦片**.  
**通过使用多个覆盖(一些横条,一些竖条,和一些正方形)我们就可以得到所有想要的特性:**:  
> 优先沿着每个维度泛化,并且拥有学习特定的组合值的能力.  

**减少内存需求的另一个有用技巧是 哈希（hashing）**:  
> 哈希产生的瓦片由随机散布于整个状态空间中的不连续且互斥的区域组成,但他们合在一起仍然覆盖了整个状态空间.   
### 9.5.6 径向基函数:  
**径向基函数（RBF）是粗编码到连续特征(实值特征的自然泛化。 相较于每个特征都是0或1，它可以是区间 [0,1] 中的任何值，反映这个"特征"出席"的程度**。   
 ![10](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/10.png)   
 
**RBF相对于二值特征的主要优点是**:  
> **它们产生的近似函数平滑变化且可微**。  
> 虽然这很吸引人，但在大多数情况下它没有实际意义。

**RBF网络的一些学习方法也改变了特征的中心和宽度，使它们进入非线性函数近似的领域。**  非线性方法可以更精确地拟合目标函数。   

 **RBF网络，特别是非线性RBF网络的缺点是**:  
>  计算复杂度更高，并且经常需要更多的手动调参来使得学习变得既鲁棒又高校。    
## 9.6 手动选择步长参数
**在表格MC方法中**产生样本平均值的经典选择 αt=1/t 不适用于TD方法，非平稳问题或任何使用函数近似的方法。     
**对于线性方法**，存在设置最佳 矩阵 步长的递归最小二乘法，并且这些方法可以扩展到时序差分学习， 但这些方法需要 O(d2) 步长参数，或者比我们学习的参数多 d 倍。 出于这个原因，我们排除在需要函数近似的大型问题上的使用。    
  
一般来说，如果 α=1/τ， 那么一个状态的**表格估计**在关于状态的 τ 次经验后将接近其目标的均值，且更近的目标影响更大。    
  
对于一般函数近似，没有关于状态的经验 数量 的这种明确概念，因为每个状态可能与所有其他状态在不同程度上相似和不相似。     

然而，在线性函数近似的情况下，存在类似的规则。   
假设你想在 τ 个经验内用大致相同的特征向量来学习。 然后，设置线性SGD方法的步长参数的一个好的经验法则是:    
![11](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/11.png)    
## 9.7 非线性函数近似：人工神经网络(ANN)   
人工神经网络（ANN）广泛用于**非线性函数近似**。   
如果ANN在其连接中至少有一个**循环**，则它是一个循环而不是前馈ANN。    
前馈和循环人工神经网络都已用于强化学习.    

**具有单个隐藏层的ANN如果包含足够大的有限数量的S形单元，可以在网络输入空间的紧凑区域上以任意精度近似任何连续函数**（Cybenko，1989）。   
对于满足适当条件的其他非线性激活函数也是如此，但**非线性是必不可少的**： 如果多层前馈ANN中的所有单元的激活函数都是线性的，则整个网络相当于没有隐藏层的网络（因为线性函数的线性函数本身是线性的）。    

**训练ANN的隐藏层是一种自动创建适合于给定问题的特征的方法，从而可以在不依赖于手工选择的特征的情况下生成分层表示的特征**。   
> 这对人工智能来说一直是一个挑战，并**解释了为什么具有隐藏层的人工神经网络的学习算法多年来受到如此多的关注**。    

人工神经网络通常通过随机梯度法学习（第9.3节）。每个权重的调整方向旨在改善网络的整体性能，如通过目标函数测量的**最小化或最大化**。   
> 在最常见的**监督学习案例中**，目标函数是一组标记的训练样例的预期误差或损失,目标就是**最小化**这个误差或损失。     
> 在**强化学习中**，人工神经网络可以使用TD误差来学习价值函数，或者他们的目标是**最大化**预期奖励，如梯度老虎机（第2.8节）或策略梯度算法（第13章）。   

对于具有隐藏层的人工神经网络（假设单元的激活函数是可导的），最成功的方法是**反向传播算法**:   
> **反向传播算法**它包括通过网络的交替的前向和后向传递。    
> 在给定网络输入单元的当前激活的情况下，每个**前向传递**计算每个单元的激活。  
> 在每次前进之后，**后向传递**有效地计算每个权重的偏导数。 （与其他随机梯度学习算法一样，这些偏导数的向量是对真实梯度的估计。）    

**反向传播算法可以为具有1或2个隐藏层的浅网络产生良好的结果，但是对于更深的ANN可能不能很好地工作**。   
> 实际上，训练具有 k+1 个隐藏层的网络实际上可能导致比使用 k 个隐藏层的网络训练更差的性能， 即使更深的网络可以代表更浅网络的所有功能（Bengio，2009）。   
> 解释这些结果并不容易，但有几个因素很重要。     
> **首先，典型深度ANN中的大量权重使得难以避免过度拟合的问题**，即，未能正确地泛化到网络未训练过的情况的问题。    
> 其次，**反向传播对于深度神经网络效果不佳，因为其向后传递计算的偏导数 要么快速衰减到网络的输入端，使得深层学习极慢，要么偏导数快速向输入侧增长，使学习不稳定**。    
> 现在深度ANN取得的一系列令人印象深刻的结果,在很大程度上源于成功解决了这些问题。    

**过拟合是任何函数逼近方法在有限的数据上训练多自由度函数都会遇到的问题.**  

**一些避免过拟合的方法:**  
> **当模型的性能开始在校验集上下降时停止训练(交叉验证).**.   
  
> **修改目标函数以限制近似函数的复杂性（正则化**）.   
  
> **引进参数减少自由度（例如，权重分享）**。    

> **一种用于减少深度神经网络过度拟合的特别有效的方法是由Srivastava,Hinton，Krizhevsky，Sutskever和Salakhutdinov（2014）引入的dropout方法**。   
> 在训练期间，单元及其连接随机从网络中删除。这可以被认为是训练大量“稀疏”网络。在测试时结合这些稀疏网络的结果是提高泛化性能的一种方法。 **一种有效的组合方法是**: 通过将单元的每个输出权重乘以训练期间保留该单元的概率来有效地近似该组合。 Srivastava等人发现这种方法显着提高了泛化性能。它鼓励个别隐藏单元学习与其他特征的随机集合配合良好的特征。 这增加了由隐藏单元形成的特征的多功能性，使得网络不会过度专注于很少发生的情况。

**解决深层人工神经网络深层训练问题:**.  
1.  Hinton，Osindero和Teh（2006）**深度置信网络:**   
> 在他们的方法中，使用无监督学习算法一次一个地训练最深层。   
> 逐层无监督训练学习使得优化是每层局部的,不依赖于整体目标函数，无监督学习可以提取捕获输入流的统计规律的特征。   
> 首先训练最深层，然后通过该训练层提供输入，训练第二深的层，依此类推，直到网络层中所有或多个层中的权值都训练了.  
> 将这些权值设置为现在作为初始的值用于监督学习。 然后通过关于整体目标函数的反向传播来微调网络。  
> 研究表明，这种方法通常比使用随机值初始化的权重的反向传播更好。   
> 以这种方式初始化的权重训练的网络的更好性能可能是由于许多因素造成的， 其中一种想法是该方法把网络的权值预训练到一个适合梯度方法的区域，基于梯度的算法可以从中获得良好的进展。   

2.**批量标准化:**  
> 批量标准化 （Ioffe和Szegedy，2015）是另一种可以更容易地训练深度人工神经网络的技术。   
> 人们早就知道，如果网络输入被标准化，则ANN学习更容易   
> 例如，通过将每个输入变量调整为具有零均值和单位方差。 用于训练深度ANN的批量标准化将深层的输出在进入下一层之前进行标准化。 Ioffe和Szegedy（2015）使用来自训练样例的子集或“小批量”的统计来归一化这些层间信号，以提高深度ANN的学习率。   

3.**深度残差学习:**   
> 另一种用于训练深度ANN的技术是 深度残差学习 （He，Zhang，Ren和Sun，2016）。  
> 有时，学习函数与恒等函数的区别比学习函数本身更容易。     
> 将该差值，即残差函数添加到输入产生所需的函数。 在深度人工神经网络中，只需在块周围添加快捷方式或跳过连接，就可以制作一个层块来学习残差函数。 这些连接将块的输入添加到其输出，并且不需要额外的权重。   
> He等人（2016）使用深度卷积网络评估该方法， 该网络具有围绕每对相邻层的跳过连接，在没有基准图像分类任务上的跳过连接的情况下发现对网络的实质性改进。    
> 批量标准化和深度残差学习都用于我们在第16章中描述的围棋游戏的强化学习应用程序。    


**深度卷积网络**:  
> 这种网络比较适合于处理高维空间数据.   
> 深度卷积网络可以直接通过反向传播进行训练，而无需采用上述方法来训练深层。   
> 深度卷积网络中的下采样层是为了降低特征图的空间分辨率.   
## 9.8 最小二乘时序差分(LSTD)  
**最小二乘时序差分基本原理:**  
![12](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/12.png)    

**LSTD复杂度:**
其计算复杂度为 O(d2)，当然保存 Aˆt 矩阵 所需的内存为 O(d2),即它的空间复杂度也为 O(d2)。   

![13](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/13.png)   


**注意:**   
> O(d2) 仍然比半梯度TD的 O(d) 消耗更多。    
>  LSTD的更高数据效率是否值得计算花费取决于 d 的大小，快速学习的重要性以及系统其他部分的费用。   

 **LSTD不需要步长参数的事实可能被夸大了**:   
 > LSTD不需要步长，但它需要设置参数 ε； 如果 ε 选择得太小，逆的序列可能变化很大，如果 ε 选择得太大，那么学习速度会慢。    
 > 此外，LSTD缺乏步长参数意味着它没有遗忘机制。这有时是可取的，但如果目标策略 π 在强化学习和GPI中发生变化则存在问题。    
 > 在控制应用中，LSTD通常必须与其他一些有遗忘机制的方法相结合，这使得它不需要步长的优势消失了。   

## 9.9 基于记忆的函数逼近  
到目前为止，我们已经讨论了近似价值函数的 参数化 方法。 在该方法中，学习算法调整用于在问题的整个状态空间上近似价值函数的函数形式的参数。   

基于记忆的函数近似方法非常不同的。**基本原理:**    
> 他们只需将训练样例保存在记忆中（或者至少保存一部分样例）而不更新任何参数。   
> 然后，每当需要查询状态的价值估计时，从记忆中检索一组样例并用于计算查询状态的价值估计。   
> 这种方法有时被称为 拖延学习，因为处理训练样例被推迟，直到查询系统才提供输出。    

**有许多不同的基于记忆的方法**:  
> 具体取决于如何选择和使用存储下来的训练样本以回应查询操作。   
1. 在这里，我们主要关注 **局部学习（local-learning） 方法**:   
> 这种方法仅仅使用查询某个状态的相邻的状态来估计其价值函数的近似值。  
>  该方法从记忆中检索一组训练样例，其状态被判断为与查询状态最相关，其中相关性通常取决于状态之间的距离： 训练样例的状态越接近查询状态，它就被认为是越相关，其中距离可以以许多不同的方式定义。在查询状态得到它的价值估计后,这个局部的近似就舍弃了.  


基于记忆的方法的最简单示例是 **最近邻居（nearest neighbor） 方法**  
> 其仅在内存中找到其状态最接近查询状态的样例，并将该样例的值作为查询状态的近似价值返回。 换句话说，如果查询状态是 s，s′↦g 是内存中的样例， 其中 s′ 是与 s 最接近的状态，则 g 作为 s 的近似价值返回。   

稍微复杂的是 **加权平均 方法**:   
> 其检索一组最近邻居样例并返回其目标值的加权平均值， 其中权重通常随着其状态与查询状态之间的距离增加而减小。    

**局部加权回归** 是类似的:   
> 但它通过参数近似方法将表面拟合一个曲面， 该方法最小化加权误差测量，如（9.1），其中权重取决于与查询状态的距离。 返回的值是在查询状态下对局部拟合曲面的求值，之后丢弃局部近似曲面。    

作为非参数的方法，**基于记忆的方法**具有优于参数方法的优点，它具有**不需要预先确定近似函数形式的优势.**。 随着更多数据的累积，这可以提高准确性。基于记忆的局部近似方法具有其他属性，使其非常适合强化学习。   

基于记忆的局部方法可以将函数逼近集中在实际或模拟轨迹中访问的状态（或状态-动作对）的局部邻域上。 **可能不需要全局近似**，因为状态空间的许多区域将永远（或几乎从未）到达。   

此外，**基于记忆的方法允许智能体对于当前状态领域的价值估计有即时的影响**,而参数化方法需要不断的增量式的调整参数来获得全局近似。

**避免全局近似也是解决维度灾难的一种方法**。  
 > 例如，对于具有 k 维的状态空间，存储全局近似的表格方法需要内存为 k 的指数的空间。  
 >  另一方面，在存储用于基于记忆的方法的样例时，每个样例需要与 k 成比例的内存， 并且存储例如 n 个样例所需的内存关于 n 是线性的。k 或 n 中没有任何指数。 当然，关键的剩余问题是基于内存的方法是否能够足够快地回答查询以对个体有用。 一个相关的问题是随着内存大小的增长速度如何降低。在大型数据库中查找最近邻居可能需要很长时间才能在许多应用程序中实用。

基于内存的方法的支持者已经开发出**加速最近邻搜索方法**:  
> 使用并行计算机或专用硬件是一种方法；    
> 另一种是使用特殊的多维数据结构来存储训练数据。   为该应用研究的一种数据结构是 k−d 树（k 维树的简称）， 其递归地将 k 维空间分成组织为二叉树的节点的区域。 根据数据量及其在状态空间中的分布情况，使用 k−d 树的最近邻搜索可以在搜索邻居时快速消除空间的大区域， 使搜索在一些简单（naive）搜索也会花费长时间的问题上变得可行。   

局部加权回归还需要快速方法来进行局部回归计算，必须重复这些回归计算以回答每个查询。 研究人员已经开发出许多方法来解决这些问题，包括为了将数据库的大小保持在一定范围内进行主动的忘记条目的方法。    
## 9.10  基于核的函数近似 
分配这些权重的函数称为 **核函数**，或简称为 核。    
k:S×S→R， 因此 k(s,s′) 是在查询状态为s时,为s′ 对于查询回复的影响分配的权值。   
从略微不同的角度来看，k(s,s′) 是从 s′ 到 s 的泛化能力的度量.  

核回归 是基于记忆的方法，它计算存储在内存中的 所有 样本的目标的核加权平均值，将结果分配给查询状态。    

一个常见的核是在第9.5.5节中描述的RBF函数近似中使用的高斯径向基函数（RBF）。   

调整中心和宽度的"去除法" ，这是一种线性参数方法.   


**使用RBF核的核回归在两个方面有所不同:**   
>  首先，它是基于记忆的：RBF以存储的样本的状态为中心。   
> 其次，它是非参数的：没有要学习的参数。   

**任何线性参数回归方法，都可以被重塑为核函数回归.**.  

**“核方法”，它允许在巨大高位的特征空间上有效地工作，而实际上只需要处理记忆中存储的训练样本。**    
 
## 9.11 深入了解在策略(同轨策略)学习：兴趣和重点
 在策略(同轨策略)分布被定义为在遵循目标策略的同时在MDP中遇到的状态分布。

**兴趣值和强调值:**   
![14](https://github.com/HTL2018/reinforcement_learning/blob/master/reinforcement_learning_an_introduction/image/Chapter_9/14.png)   

## 9.12 本章小结  
**也许最合适的监督学习方法是使用 参数化函数近似 的方法**:   
> 其中策略由权向量 w 参数化。 虽然权重向量有很多分量，但状态空间仍然大得多，我们必须找到一个近似的解决方案。 我们将 均方值误差 VE¯(w) 定义为 在策略分布 μ 下 权重向量 w 的值 vπw(s) 中的误差的度量。 VE¯ 为我们提供了一种明确的方法，可以在在策略案例中对不同的价值函数近似进行排序。   

本章中，我们重点关注具有 固定策略 的 在策略 案例，也称为策略评估或预测；   

权重向量出现在更新目标中，但在计算梯度时不考虑这一点──因此它们是 **半梯度方法**。    

**选择特征**是将先前领域知识添加到强化学习系统的最重要方法之一。   
 > 它们可以被选择为**多项式**，但是这种情况在通常在强化学习中考虑的在线学习设置中普遍效果不好。   
> 更好的是根据**傅立叶基**选择特征，或者根据稀疏重叠感受域的某种形式的粗编码。   
> **瓦片编码**是粗编码的一种形式，其特别具有计算效率和灵活性。    
> **径向基函数**对于一维或二维任务非常有用，其中平滑变化的响应很重要。    
>  **LSTD**是数据最有效的线性TD预测方法，但需要与权重数的平方成比例的计算，而所有其他方法在权重数量上具有线性复杂性。  
>  非线性方法包括通过反向传播训练的人工神经网络和SGD的变化；这些方法近年来以 **深度强化学习**的名义变得非常流行。

对于所有 n，线性半梯度n步TD保证在标准条件下 收敛到最佳误差范围内的 VE¯（通过蒙特卡罗方法渐近实现）。 这个边界会随着n的增大而减小，并且对于 n→∞ 来说接近零。 然而，在实践中，非常高的n导致学习非常缓慢，并且一定程度的自举（n<∞）通常是可取的， 正如我们在第7章中的表格n步法和和第5章中表格TD和蒙特卡罗方法的比较中所看到的那样。    

[Sherman-Morrison Woodbury 证明](https://www.vartang.com/2014/10/smw-formula/)    

Saturday, 07. December 2019 04:48PM 
